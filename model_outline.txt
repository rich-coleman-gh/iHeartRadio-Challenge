For the sake of clarity I will begin by outlining the general steps that should be taken when building any model. I will go into further detail with a specific plan of action regarding option "C"

1. Define your objective.
    -It is often overlooked, but it is extremely important to understand what it is you are trying to predict and why you are trying to predict it. This will often help you decide what approach you should take (which model you should be building off of the data you have).
2. Collect your data.
    - Collecting your data does not just mean pulling it all into one place. It is important to structure your data in a way that makes sense and aligns with the model you have chosen to build off of. For example if we are trying to predict off of a user basis we might want to aggregate our data such that each row in our table represents one user with aggregated metrics regarding each user. 
3. Prepare your data.
    - More often than not data will come in a state that is very raw, where nulls and outliers are often present. That is not to say we want to remove outliers and null values, but instead we might want to take a derived variable of the variable with outliers. For example we could take the log(xn) such that the difference between the normal values and the outlier is reduced.
4. Feature Selection
 - There are many ways to go about deciding what features should and should not be used in your model, but regardless of the method we must decide what variables are "important" or related to the dependent variable we are trying to predict. The methods here vary wildly, as sometimes the relationship between dependent and independent variable is linear, while other times it is not. If our model is trying to fit our data linearly we might want to try feature selection through a linear method (Pearsonâ€™s coefficient) and vice versa for a non-linear method (Mutual Information).
5. Variable Reduction (Not always required)
    - Depending on the size of our dataset, we sometimes have a large number of features that can make building a model difficult. It can be helpful to reduce the number of features in our dataset by creating a weighted vector of like features (for example through PCA). I say this is optional because there is a trade-off that depends on the objective your model is trying to accomplish. Feature reduction often times obscures interpretability while helping boost prediction power.
6. Creating a training and holdout set of your data.
    - One of the biggest mistakes that can be made when constructing a model is building your model off the same set of data you are evaluating on. Cross validating is extremely important when evaluating how well your model generalizes to other data, and also helps prevent us from over fitting our data.
7. Evaluating your model (training set).
    - This step can vary greatly depending on the model you have decided to build off of your data. The evaluation method is specific to the class of model that you are building. For example the predictive ability of a logistic regression could be examined through a combination of the models ROC curve, residuals, and p values of each factor in the model.
8. Cross Validation (test set)
    - Once you have a good measure of the predictive power of your model it is important to see how generalizable its predictions are on other datasets. This helps guard against over fitting, which certain models (such as tree methods) are prone to.
   
##########################################################Option C#########################################################
{Please note that while I will be going over each step of the process in detail, I will try and be succinct with reference to the steps I have outlined above}

This write-up is meant to serve as a post-mortem of my first pass at modeling the problem stated in option C. This is by no means the final solution to option C, but instead an attempt at establishing a solid baseline to build off of through iteration. To begin I first needed to define what it means for an artist to be "trending". After examining the structure of my data I saw that I could use the number of times an artist had been listened to as a measure of how popular they are. It would have been ideal if I had access to a users social data (i.e. who are they friends with) as I imagine trending artists can spread through social networks, or perhaps the music label the artist first uses to expose themselves to listeners. Ideally we would also have user voting/preference data that we could factor into an artists "trendiness". 

I then began examining my data and looked at the distributions of each feature of my dataset. I noticed that my dataset contained both NULL values as well as outliers. I then began to create derived features from each column of interest (such as age) where I would take the log/square root or exclude the null values from the feature. After cleaning my dataset and creating a derived feature set, I set out on deciding what class of model I wanted to use to tackle this problem. I wanted a model that was easy to interpret, fast to deploy, and was quick to develop. Since our dependent variable we are trying to predict is number of times listened to, and thus continuous, I went with trying to develop a linear regression. While I note that I am sacrificing accuracy by using a linear regression I am gaining interpretability and speed of production. More complex models can be built later in the future that will set the new baseline to beat. It is extremely important to highlight that there is no perfect model to apply to all problems. Each model comes with its own advantages and trade offs that must be decided by the modeler. My intent is not to create a black box that has extremely accurate predictions. Instead I want a model that is strong at predicting a trending artist but can also be explained to those without strong modeling backgrounds (executives) which could help expedite decision making across the company.

Once I decided on what class of model to build, I knew that any interaction the linear regression could pick up would be linear. This helped me in my next step of selecting a feature set to put in the model. Before doing any actual feature selection, I first had to create a training and a test set from my data. Once these sets were created I then set out to build my feature set off the training set I created.

I could have manually tried to select the optimal set of features through pearsons/spearmans coefficients, but I instead chose to use a stepwise regression (backwards elimination).This would use every feature in my model and recursively remove one feature at a time (only keeping the feature if an improvement in performance is made). Since I am trying to establish a baseline to improve upon through iteration this seemed like a wise starting point.

After I had selected my features I next wanted to evaluate my linear regressions predictive power. By looking at a combination of the R-squared, standard deviation of the residuals, as well as inspecting the residuals visually I then was able to determine whether or not my model accurately was predicting the data. By looking at the residuals I also was able to ensure that the data I was trying to fit was indeed linear. If I had found that the relationship I was trying to model was non-linear, I would have had to rethink the model I was using and try a non-linear approach (such as a random forest). I also made sure to inspect the p-values of each independent variable as well as the coefficients of each variable to make sure that the values were in line with the overall objective I was trying to accomplish. For example if we saw that region was the most predictive feature in predicting trendiness, we would want to consider other confounding factors that could be influencing our results. Perhaps the region of interest has the largest number of music listeners and thus a regions population should be accounted for in the model.

After evaluating my model against my training set it was finally time to validate against my holdout set of data. I saw that my linear regression generalized well against my test set and maintained similar R-squared values as well as residuals. I was pleased with my first pass at predicting if a user was going to "trend", but it is important to call out that this can easily be improved upon. Perhaps my rudimentary definition of "trending" can be refined and improved upon with more data. Perhaps I can collect more variables that might prove to be relevant to the target variable I am trying to predict. Perhaps I can obtain data from outside sources such as social networks as well as more data on my collection of artists. This merely provides a foundation of performance for which the next model can improve upon. 
